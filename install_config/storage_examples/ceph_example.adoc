[[install-config-storage-examples-ceph-example]]
<<<<<<< HEAD
= Using Ceph RBD for persistent storage
=======
= Complete Example Using Ceph RBD
>>>>>>> 6bcfc60496ce2d67bb2988b30404317813a382f5
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

<<<<<<< HEAD
This topic provides a complete example of using an existing Ceph cluster for
{product-title} persistent storage. It is assumed that a working Ceph cluster is
already set up. If not, consult the
=======
This topic provides an end-to-end example of using an existing Ceph cluster as
an {product-title} persistent store. It is assumed that a working Ceph cluster
is already set up. If not, consult the
>>>>>>> 6bcfc60496ce2d67bb2988b30404317813a382f5
link:https://access.redhat.com/products/red-hat-ceph-storage[Overview of Red Hat
Ceph Storage].

xref:../persistent_storage/persistent_storage_ceph_rbd.adoc#install-config-persistent-storage-persistent-storage-ceph-rbd[Persistent Storage
Using Ceph Rados Block Device] provides an explanation of persistent volumes
(PVs), persistent volume claims (PVCs), and using Ceph RBD as persistent
storage.

[NOTE]
====
<<<<<<< HEAD
* Run all `oc` commands on the {product-title} master host.
* The {product-title} all-in-one host is not often used to run pod workloads and,
thus, is not included as a schedulable node.
====

[[using-existing-ceph-cluster-as-persistent-store]]
== Using an existing Ceph cluster for persistent storage

To use an existing Ceph cluster for persistent storage:

. Install the latest ceph-common package:
+
[source, bash]
----
yum install -y ceph-common
----
+
[NOTE]
====
The *ceph-common* library must be installed on *all schedulable* {product-title}
nodes.
====

.  Create the keyring for the client:
+
[source, bash]
----
$ ceph auth get-or-create client.openshift-test mon 'allow *' osd 'allow rwx pool=openshift' -o /etc/ceph/ceph.client.openshift-test-keyring
----

. Convert the keyring to base64:
+
[source, bash]
----
$ /etc/ceph/ceph.client.openshift-test-keyring
[client.openshift-test]
	key = AQBO2xdbq3iTDhAAWCo8loVA5RitlF4Sde5uYQ==
$ echo -n AQBO2xdbq3iTDhAAWCo8loVA5RitlF4Sde5uYQ== |base64
QVFCTzJ4ZGJxM2lURGhBQVdDbzhsb1ZBNVJpdGxGNFNkZTV1WVE9PQ==
----

. Edit the *_ceph-secret.yaml_* file to include the base64-decoded keyring:
+
.Ceph secret definition example
=======
All `oc ...` commands are executed on the {product-title} master host.
====

[[using-ceph-rbd-installing-the-ceph-common-package]]
== Installing the ceph-common Package
The *ceph-common* library must be installed on *all schedulable* {product-title} nodes:

[NOTE]
====
The {product-title} all-in-one host is not often used to run pod workloads and,
thus, is not included as a schedulable node.
====

----
# yum install -y ceph-common
----

[[using-ceph-rbd-creating-the-ceph-secret]]
== Creating the Ceph Secret

The `ceph auth get-key` command is run on a Ceph *MON* node to display the key
value for the *client.admin* user:

.Ceph Secret Definition
====
>>>>>>> 6bcfc60496ce2d67bb2988b30404317813a382f5
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
<<<<<<< HEAD
  name: ceph-secret-test
data:
  key: QVFCTzJ4ZGJxM2lURGhBQVdDbzhsb1ZBNVJpdGxGNFNkZTV1WVE9PQo= <1>
----
<1> This base64 key is generated on one of the Ceph MON nodes using the `ceph
auth get-key client.admin | base64` command, then copying the output and pasting
it as the secret keyâ€™s value.

. Create the secret:
+
[source, bash]
----
$ oc create -f ceph-secret.yaml 
secret "ceph-secret-test" created
----

. Verify that the secret was created:
+
[source, bash]
----
$ oc get secret
NAME                       TYPE          DATA      AGE
ceph-secret-test           Opaque        1         22h
----

. Create the PV object definition using Ceph RBD:
+
.PV object definition using Ceph RDB example
=======
  name: ceph-secret
data:
  key: QVFBOFF2SlZheUJQRVJBQWgvS2cwT1laQUhPQno3akZwekxxdGc9PQ== <1>

----
<1> This base64 key is generated on one of the Ceph MON nodes using the `ceph auth get-key client.admin | base64` command, then copying the output and pasting it as the secret key's value.
====

Save the secret definition to a file, for example *_ceph-secret.yaml_*,
then create the secret:

====
----
$ oc create -f ceph-secret.yaml
secret "ceph-secret" created
----
====

Verify that the secret was created:

====
----
# oc get secret ceph-secret
NAME          TYPE      DATA      AGE
ceph-secret   Opaque    1         23d
----
====

[[using-ceph-rbd-creating-the-persistent-volume]]
== Creating the Persistent Volume

Next, before creating the PV object in {product-title}, define the persistent
volume file:

.Persistent Volume Object Definition Using Ceph RBD
====

>>>>>>> 6bcfc60496ce2d67bb2988b30404317813a382f5
[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
<<<<<<< HEAD
  name: ceph-pv-test    <1>
=======
  name: ceph-pv     <1>
>>>>>>> 6bcfc60496ce2d67bb2988b30404317813a382f5
spec:
  capacity:
    storage: 2Gi    <2>
  accessModes:
    - ReadWriteOnce <3>
  rbd:              <4>
    monitors:       <5>
      - 192.168.122.133:6789
    pool: rbd
<<<<<<< HEAD
    image: ceph-image <6>
    user: admin
    secretRef:
      name: ceph-secret-test <7>
    fsType: ext4        <8>
=======
    image: ceph-image
    user: admin
    secretRef:
      name: ceph-secret <6>
    fsType: ext4        <7>
>>>>>>> 6bcfc60496ce2d67bb2988b30404317813a382f5
    readOnly: false
  persistentVolumeReclaimPolicy: Retain
----
<1> The name of the PV, which is referenced in pod definitions or displayed in
various `oc` volume commands.
<2> The amount of storage allocated to this volume.
<<<<<<< HEAD
<3> The `*accessModes*` are used as labels to match a PV and a PVC. They currently
=======
<3> `*accessModes*` are used as labels to match a PV and a PVC. They currently
>>>>>>> 6bcfc60496ce2d67bb2988b30404317813a382f5
do not define any form of access control. All block storage is defined to be
single user (non-shared storage).
<4> This defines the volume type being used. In this case, the *rbd* plug-in is
defined.
<5> This is an array of Ceph monitor IP addresses and ports.
<<<<<<< HEAD
<6> The `ceph-image` must be created on the Ceph cluster.
<7> Enter the Ceph secret that you created. It is used to create a secure
connection from {product-title} to the Ceph server.
<8> This is the type of file system that is mounted on the Ceph RBD block device.

. Create the PV:
+
[source, bash]
----
$ oc create -f ceph-pv-test.yaml
persistentvolume "ceph-pv-test" created
----

. Verify that the PV was created:
+
[source, bash]
----
$ oc get pv
NAME                     LABELS    CAPACITY     ACCESSMODES   STATUS      CLAIM     REASON    AGE
ceph-pv                  <none>    2147483648   RWO           Available                       2s
----

. Create the PVC object definition:
+
.PVC object definition example
=======
<6> This is the Ceph secret, defined above. It is used to create a secure
connection from {product-title} to the Ceph server.
<7> This is the file system type mounted on the Ceph RBD block device.
====

Save the PV definition to a file, for example *_ceph-pv.yaml_*,
and create the persistent volume:

====
----
# oc create -f ceph-pv.yaml
persistentvolume "ceph-pv" created
----
====

Verify that the persistent volume was created:

====
----
# oc get pv
NAME                     LABELS    CAPACITY     ACCESSMODES   STATUS      CLAIM     REASON    AGE
ceph-pv                  <none>    2147483648   RWO           Available                       2s
----
====

[[using-ceph-rbd-creating-the-persistent-volume-claim]]
== Creating the Persistent Volume Claim
A persistent volume claim (PVC) specifies the desired access mode and storage
capacity. Currently, based on only these two attributes, a PVC is bound to a
single PV. Once a PV is bound to a PVC, that PV is essentially tied to the PVC's
project and cannot be bound to by another PVC. There is a one-to-one mapping of
PVs and PVCs. However, multiple pods in the same project can use the same PVC.

.PVC Object Definition
====
>>>>>>> 6bcfc60496ce2d67bb2988b30404317813a382f5
[source,yaml]
----
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
<<<<<<< HEAD
  name: ceph-claim-test
spec:
  accessModes: <1>
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi <.2>
----
<1> The `accessModes` do not enforce access rights but instead act as labels to match a PV to a PVC.
<2> This claim looks for PVs that offer 2Gi or greater capacity.

. Create the PVC:
+
[source, bash]
----
$  oc create -f ceph-claim-test.yaml
persistentvolumeclaim "ceph-claim-test" created
----

. Verify that the PVC was created and bound to the expected PV:
+
[source, bash]
----
$ oc get pvc
NAME              STATUS    VOLUME         CAPACITY   ACCESSMODES   STORAGECLASS   AGE
ceph-claim-test   Bound     ceph-pv-test   2Gi        RWO                          8s
----

. Create the pod object definition:
+
.Pod object definition example
=======
  name: ceph-claim
spec:
  accessModes:     <1>
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi <2>

----
<1> As mentioned above for PVs, the `*accessModes*` do not enforce access right,
but rather act as labels to match a PV to a PVC.
<2> This claim will look for PVs offering *2Gi* or greater capacity.
====

Save the PVC definition to a file, for example *_ceph-claim.yaml_*,
and create the PVC:

====
----
# oc create -f ceph-claim.yaml
persistentvolumeclaim "ceph-claim" created

#and verify the PVC was created and bound to the expected PV:
# oc get pvc
NAME         LABELS    STATUS    VOLUME    CAPACITY   ACCESSMODES   AGE
ceph-claim   <none>    Bound     ceph-pv   1Gi        RWX           21s
                                 <1>
----
<1> the claim was bound to the *ceph-pv* PV.
====

[[using-ceph-rbd-creating-the-pod]]
== Creating the Pod
A pod definition file or a template file can be used to define a pod. Below is a
pod specification that creates a single container and mounts the Ceph RBD volume
for read-write access:

.Pod Object Definition
====
>>>>>>> 6bcfc60496ce2d67bb2988b30404317813a382f5
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: ceph-pod1           <1>
spec:
  containers:
  - name: ceph-busybox
    image: busybox          <2>
    command: ["sleep", "60000"]
    volumeMounts:
    - name: ceph-vol1       <3>
      mountPath: /usr/share/busybox <4>
      readOnly: false
  volumes:
  - name: ceph-vol1         <3>
    persistentVolumeClaim:
      claimName: ceph-claim <5>
----
<1> The name of this pod as displayed by `oc get pod`.
<<<<<<< HEAD
<2> The image run by this pod. In this example, `busybox` is set to `sleep`.
<3> The name of the volume. This name must be the same in both the `*containers*` and `*volumes*` sections.
<4> The mount path as seen in the container.
<5> The PVC bound to the Ceph RBD cluster.

. Create the pod:
+
[source, bash]
----
$ oc create -f ceph-pod-test.yaml
pod "ceph-pod-test" created
----

. Verify that the pod was created:
+
[source, bash]
----
$ oc get pod
NAME        READY     STATUS    RESTARTS   AGE
ceph-pod1   1/1       Running   0          2m
----

After a minute or so, the pod status changes to *Running*.

[[using-ceph-rbd-defining-group-and-owner-ids-optional]]
== Defining group and owner IDs (Optional)
=======
<2> The image run by this pod. In this case, we are telling *busybox* to sleep.
<3> The name of the volume. This name must be the same in both the `*containers*` and `*volumes*` sections.
<4> The mount path as seen in the container.
<5> The PVC that is bound to the Ceph RBD cluster.
====

Save the pod definition to a file, for example *_ceph-pod1.yaml_*,
and create the pod:

====
----
# oc create -f ceph-pod1.yaml
pod "ceph-pod1" created

#verify pod was created
# oc get pod
NAME        READY     STATUS    RESTARTS   AGE
ceph-pod1   1/1       Running   0          2m
                      <1>
----
<1> After a minute or so, the pod will be in the *Running* state.
====

[[using-ceph-rbd-defining-group-and-owner-ids-optional]]
== Defining Group and Owner IDs (Optional)
>>>>>>> 6bcfc60496ce2d67bb2988b30404317813a382f5
When using block storage, such as Ceph RBD, the physical block storage is
*managed* by the pod. The group ID defined in the pod becomes the group ID of
*both* the Ceph RBD mount inside the container, and the group ID of the actual
storage itself. Thus, it is usually unnecessary to define a group ID in the pod
specifiation. However, if a group ID is desired, it can be defined using
`*fsGroup*`, as shown in the following pod definition fragment:

<<<<<<< HEAD
.Group ID pod definition example
=======
.Group ID Pod Definition
====
>>>>>>> 6bcfc60496ce2d67bb2988b30404317813a382f5
[source,yaml]
----
...
spec:
  containers:
    - name:
    ...
  securityContext: <1>
    fsGroup: 7777  <2>
...
----
<1> `securityContext` must be defined at the pod level, not under a specific container.
<2> All containers in the pod will have the same `*fsGroup*` ID.
<<<<<<< HEAD

[[using-ceph-rbd-setting-default-secret]]
== Setting ceph-user-secret as the default for projects

To make the persistent storage available to every project, you need to modify
the default project template. Adding this to your default project template
allows every user who has access to create a project access to the Ceph cluster.
See
xref:../../admin_guide/managing_projects.adoc#selfprovisioning-projects[modifying
the default project template] for more information.

.Default project example
=======
====

[[using-ceph-rbd-setting-default-secret]]
== Setting ceph-user-secret as Default for Projects

If you would like to make the persistent storage available to every project you have to modify the default project template.
You can read more on modifying the default project template. Read more on xref:../../admin_guide/managing_projects.adoc#selfprovisioning-projects[modifying the default project template].
Adding this to your default project template allows every user who has access to create a project access to the Ceph cluster.

.Default Project Example
>>>>>>> 6bcfc60496ce2d67bb2988b30404317813a382f5
[source,yaml]
----
...
apiVersion: v1
kind: Template
metadata:
  creationTimestamp: null
  name: project-request
objects:
- apiVersion: v1
  kind: Project
  metadata:
    annotations:
      openshift.io/description: ${PROJECT_DESCRIPTION}
      openshift.io/display-name: ${PROJECT_DISPLAYNAME}
      openshift.io/requester: ${PROJECT_REQUESTING_USER}
    creationTimestamp: null
    name: ${PROJECT_NAME}
  spec: {}
  status: {}
- apiVersion: v1
  kind: Secret
  metadata:
    name: ceph-user-secret
  data:
    key: yoursupersecretbase64keygoeshere <1>
  type:
    kubernetes.io/rbd
...
----
<1> Place your Ceph user key here in base64 format. 
