[[install-config-configuring-nsx-t-sdn]]
= Configuring NSX-T SDN
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

[[nsx-t-sdn-and-openshift]]
== NSX-T SDN and {product-title}

VMware NSX-T provides advanced software-defined networking (SDN), security and visibility
to container environments that simplifies IT operations and extends {product-title}’s native
networking capabilities.

NSX-T supports Virtual Machine, Baremetal, and container workloads across multiple clusters. This allows
organizations to have single pane of glass across the entire environment.


See
xref:../architecture/networking/network_plugins.adoc#nsx-sdn[Networking]
for more information on how NSX-T is integrated with {product-title}.

[[nsx-t-sdn-operations-workflow]]
== Example Topology

There is a T0 router on the top that connects physical with virtual world. We also have T1 router acting as a default gateway for the Openshift VMs. Those VMs have two vNICs each. One vNIC is connected to Management Logical Switch for accessing the VMs. The second vNIC is connected to a dump Logical Switch and is used by nsx-node-agent to uplink the POD networking. The LoadBalancer used for configuring Openshift Routes plus all project’s T1 routers and Logical Switches are created automatically later when we install Openshift. In this topology we use the default Openshift HAProxy Router for all infrastructure components such as Grafana, Prometheus, Console, Service Catalog and others.
This means that the DNS records for the infra components need to point the infrastructure node IP addresses since the HAProxy uses the host network namespace.
This works well for infra routes but in order to avoid exposing the infra nodes management IPs to the outside world we will be deploying application specific routes to the NSX-T LoadBalancer.
The topology here assumes 3 x Openshift master virtual machines and 4 x Openshift worker virtual machines(two for infrastructure and two for compute).

[[nsx-t-sdn-installation]]

== Installation

Prerequisites
ESXi hosts requirements
ESXi servers that host {product-title} node VMs must be NSX-T Transport Nodes.

Virtual Machine requirements
{product-title} node VMs must have two vNICs:
1.	Management vNIC connected to the Logical Switch that is uplinked to the management T1 router.
2.	The second vNIC on all VMs needs to have two Tags in NSX in order nsx-container-plugin(NCP) to know which port needs to be used as a parent VIF for all PODs running in the particular openshift node.

** GRAPHIC IN HERE **

Tags need to be as following:

1	{'ncp/node_name':  'node_name'}
2 {'ncp/cluster': 'cluster_name'}

** Graphics in here **

* The order in the UI is reverse than in the API.
The node_name must be exactly as kubelet will see it and the cluster name must be the same as specified as nsx_openshift_cluster_name in the ansible hosts file shown below.
We need to make sure that the proper tags are applied on the second vNIC on every node.

NSX-T requirements

The following objects need to be pre-created in NSX in order later to be referred in the ansible hosts file:
1.	T0 Router
2.	Overlay Transport Zone
3.	IP Block for POD networking
4.	IP Block for routed(NoNAT) POD networking – optional
5.	IP Pool for SNAT – by default the subnet given per Project from the IP Block in point 3 is routable only inside NSX. NCP uses this IP Pool in order to provide connectivity to the outside.
6.	Top and Bottom FW sections (optional) in dFW. NCP will be placing k8s Network Policy rules between those two sections.
7.	Openvswitch and CNI plugin RPMs need to be hosted on a HTTP server reachable from the Openshift Node VMs (http://1.1.1.1 in this example).


In the Ansible nodes file, the following parameters need to be specified in
order to setup NSX-T as the network plug-in:

----
[OSEv3:children]
masters
nodes
etcd

[OSEv3:vars]
ansible_ssh_user=root
openshift_deployment_type=origin

openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]
openshift_master_htpasswd_users={'yasen' : '$apr1$dNJrJ/ZX$VvO7eGjJcYbufQkY6nc4x/'}

openshift_master_default_subdomain=demo.corp.local
openshift_use_nsx=true
os_sdn_network_plugin_name=cni
openshift_use_openshift_sdn=false
openshift_node_sdn_mtu=1500
openshift_master_cluster_method=native
openshift_master_cluster_hostname=master1.corp.local
openshift_master_cluster_public_hostname=master1.corp.local
# NSX specific configuration
#nsx_use_loadbalancer=false
nsx_openshift_cluster_name='cl1'
nsx_api_managers='192.168.110.202'
nsx_api_user='admin'
nsx_api_password='VMware1!VMware1!'
nsx_tier0_router='DefaultT0'
nsx_overlay_transport_zone='overlay-tz'
nsx_container_ip_block='pod-networking'
nsx_no_snat_ip_block='nonat-pod-networking'
nsx_external_ip_pool='external-pool'
nsx_top_fw_section='containers-top'
nsx_bottom_fw_section='containers-bottom'
nsx_ovs_uplink_port='ens224'
nsx_cni_url='http://1.1.1.1/nsx-cni-2.4.0.x86_64.rpm'
nsx_ovs_url='http://1.1.1.1/openvswitch-2.10.2.rhel76-1.x86_64.rpm'
nsx_kmod_ovs_url='http://1.1.1.1/kmod-openvswitch-2.10.2.rhel76-1.el7.x86_64.rpm'

[masters]
master1.corp.local
master2.corp.local
master3.corp.local

[etcd]
master1.corp.local
master2.corp.local
master3.corp.local

[nodes]
master1.corp.local ansible_ssh_host=10.0.0.11 openshift_node_group_name='node-config-master'
master2.corp.local ansible_ssh_host=10.0.0.12 openshift_node_group_name='node-config-master'
master3.corp.local ansible_ssh_host=10.0.0.13 openshift_node_group_name='node-config-master'
node1.corp.local ansible_ssh_host=10.0.0.21 openshift_node_group_name='node-config-infra'
node2.corp.local ansible_ssh_host=10.0.0.22 openshift_node_group_name='node-config-infra'
node3.corp.local ansible_ssh_host=10.0.0.23 openshift_node_group_name='node-config-compute'
node4.corp.local ansible_ssh_host=10.0.0.24 openshift_node_group_name='node-config-compute'
----
$ ansible-playbook -i hosts openshift-ansible/playbooks/prerequisites.yml

Once the above playbook finish, do the following on all nodes:
# Assuming NCP Container image is downloaded locally on all nodes
$ docker load -i nsx-ncp-rhel-xxx.tar

# Get the image name
$ docker images
$ docker image tag registry.local/xxxxx/nsx-ncp-rhel nsx-ncp

Last step is to deploy the Openshift cluster:
$	ansible-playbook -i hosts openshift-ansible/playbooks/deploy_cluster.yml

Once it is complete validate that the NCP and nsx-node-agent PODs are running:
$ oc get pods -o wide -n nsx-system

Check NSX-T routing section:

** Insert Graphic Here **
